Expected logs in the elastic search

minikube logs 

: connect: connection refused"
W1203 14:31:01.806484       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:01.812580       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:01.868690       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:01.910214       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.017013       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.031385       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.061733       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.077752       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.099853       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.099939       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.114197       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.176110       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.246494       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.300374       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.322192       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.386254       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.418263       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.474255       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.486368       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.496343       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.514193       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.622205       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.652209       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.662174       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.824256       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.830023       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.864353       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.925359       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.938199       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.940103       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.948312       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:02.974333       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:03.030172       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 14:31:03.092623       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [c0c83f5416d5] <==
W1203 14:31:25.511336       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1203 14:31:25.511345       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1203 14:31:25.511927       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1203 14:31:25.511951       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1203 14:31:25.522437       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1203 14:31:25.522471       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1203 14:31:25.881734       1 secure_serving.go:213] Serving securely on [::]:8443
I1203 14:31:25.881803       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1203 14:31:25.882197       1 controller.go:78] Starting OpenAPI AggregationController
I1203 14:31:25.882292       1 local_available_controller.go:156] Starting LocalAvailability controller
I1203 14:31:25.882308       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1203 14:31:25.882326       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1203 14:31:25.882373       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1203 14:31:25.882395       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1203 14:31:25.882407       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1203 14:31:25.882421       1 aggregator.go:169] waiting for initial CRD sync...
I1203 14:31:25.881795       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1203 14:31:25.883913       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1203 14:31:25.883935       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1203 14:31:25.884373       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1203 14:31:25.884484       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1203 14:31:25.885183       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1203 14:31:25.885204       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1203 14:31:25.885257       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1203 14:31:25.885282       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1203 14:31:25.885319       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1203 14:31:25.885395       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1203 14:31:25.885561       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1203 14:31:25.885719       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1203 14:31:25.885929       1 controller.go:119] Starting legacy_token_tracking_controller
I1203 14:31:25.886024       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1203 14:31:25.886149       1 controller.go:142] Starting OpenAPI controller
I1203 14:31:25.886247       1 controller.go:90] Starting OpenAPI V3 controller
I1203 14:31:25.886393       1 naming_controller.go:294] Starting NamingConditionController
I1203 14:31:25.886477       1 establishing_controller.go:81] Starting EstablishingController
I1203 14:31:25.888164       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1203 14:31:25.888172       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1203 14:31:25.888180       1 crd_finalizer.go:269] Starting CRDFinalizer
I1203 14:31:25.899807       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1203 14:31:25.931067       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1203 14:31:25.969493       1 shared_informer.go:320] Caches are synced for node_authorizer
I1203 14:31:25.976264       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1203 14:31:25.976292       1 policy_source.go:224] refreshing policies
I1203 14:31:25.982915       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1203 14:31:25.988538       1 cache.go:39] Caches are synced for LocalAvailability controller
I1203 14:31:25.994986       1 shared_informer.go:320] Caches are synced for configmaps
I1203 14:31:25.995159       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1203 14:31:25.995175       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1203 14:31:25.998145       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1203 14:31:25.995211       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1203 14:31:25.995232       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1203 14:31:25.995939       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1203 14:31:26.001096       1 aggregator.go:171] initial CRD sync complete...
I1203 14:31:26.001113       1 autoregister_controller.go:144] Starting autoregister controller
I1203 14:31:26.001122       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1203 14:31:26.001129       1 cache.go:39] Caches are synced for autoregister controller
I1203 14:31:26.044899       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1203 14:31:26.897158       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1203 14:31:42.415083       1 controller.go:615] quota admission added evaluator for: endpoints
I1203 14:31:42.565798       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-controller-manager [31a4ee8ab604] <==
I1203 14:31:42.123205       1 controllermanager.go:797] "Started controller" controller="garbage-collector-controller"
I1203 14:31:42.125955       1 garbagecollector.go:146] "Starting controller" logger="garbage-collector-controller" controller="garbagecollector"
I1203 14:31:42.125984       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1203 14:31:42.126011       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I1203 14:31:42.131863       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I1203 14:31:42.154074       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1203 14:31:42.161797       1 shared_informer.go:320] Caches are synced for service account
I1203 14:31:42.161817       1 shared_informer.go:320] Caches are synced for cronjob
I1203 14:31:42.162371       1 shared_informer.go:320] Caches are synced for TTL
I1203 14:31:42.162417       1 shared_informer.go:320] Caches are synced for TTL after finished
I1203 14:31:42.167064       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1203 14:31:42.168596       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1203 14:31:42.169914       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1203 14:31:42.170074       1 shared_informer.go:320] Caches are synced for stateful set
I1203 14:31:42.172143       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1203 14:31:42.182823       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1203 14:31:42.192247       1 shared_informer.go:320] Caches are synced for endpoint
I1203 14:31:42.192334       1 shared_informer.go:320] Caches are synced for persistent volume
I1203 14:31:42.195728       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1203 14:31:42.196097       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1203 14:31:42.202092       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1203 14:31:42.210101       1 shared_informer.go:320] Caches are synced for ReplicationController
I1203 14:31:42.211656       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1203 14:31:42.211867       1 shared_informer.go:320] Caches are synced for namespace
I1203 14:31:42.219187       1 shared_informer.go:320] Caches are synced for PVC protection
I1203 14:31:42.219415       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1203 14:31:42.221058       1 shared_informer.go:320] Caches are synced for expand
I1203 14:31:42.221258       1 shared_informer.go:320] Caches are synced for crt configmap
I1203 14:31:42.221270       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1203 14:31:42.224070       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1203 14:31:42.224906       1 shared_informer.go:320] Caches are synced for PV protection
I1203 14:31:42.230645       1 shared_informer.go:320] Caches are synced for daemon sets
I1203 14:31:42.231740       1 shared_informer.go:320] Caches are synced for node
I1203 14:31:42.232166       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1203 14:31:42.232390       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1203 14:31:42.232513       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1203 14:31:42.232605       1 shared_informer.go:320] Caches are synced for cidrallocator
I1203 14:31:42.234618       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1203 14:31:42.236063       1 shared_informer.go:320] Caches are synced for HPA
I1203 14:31:42.255861       1 shared_informer.go:320] Caches are synced for job
I1203 14:31:42.257237       1 shared_informer.go:320] Caches are synced for GC
I1203 14:31:42.257423       1 shared_informer.go:320] Caches are synced for taint
I1203 14:31:42.257942       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1203 14:31:42.258050       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1203 14:31:42.258097       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1203 14:31:42.261923       1 shared_informer.go:320] Caches are synced for ephemeral
I1203 14:31:42.288314       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1203 14:31:42.418198       1 shared_informer.go:320] Caches are synced for disruption
I1203 14:31:42.418264       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1203 14:31:42.418813       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="monitoring/kibana-68cb46cb69" duration="461.061µs"
I1203 14:31:42.429610       1 shared_informer.go:320] Caches are synced for attach detach
I1203 14:31:42.436024       1 shared_informer.go:320] Caches are synced for resource quota
I1203 14:31:42.456423       1 shared_informer.go:320] Caches are synced for resource quota
I1203 14:31:42.477331       1 shared_informer.go:320] Caches are synced for deployment
I1203 14:31:42.737352       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="319.052997ms"
I1203 14:31:42.737707       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="62.858µs"
I1203 14:31:42.910934       1 shared_informer.go:320] Caches are synced for garbage collector
I1203 14:31:42.926684       1 shared_informer.go:320] Caches are synced for garbage collector
I1203 14:31:42.926723       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1203 14:35:28.765221       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [4274de8b34d6] <==
I1203 14:31:07.852983       1 serving.go:386] Generated self-signed cert in-memory
I1203 14:31:08.080118       1 controllermanager.go:197] "Starting" version="v1.31.0"
I1203 14:31:08.080135       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 14:31:08.081141       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1203 14:31:08.081141       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1203 14:31:08.081239       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1203 14:31:08.081309       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1203 14:31:18.083158       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://192.168.49.2:8443/healthz\": dial tcp 192.168.49.2:8443: connect: connection refused"


==> kube-proxy [5f8cb22bdf28] <==
I1203 14:31:07.069741       1 server_linux.go:66] "Using iptables proxy"
E1203 14:31:07.196766       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1203 14:31:08.316096       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1203 14:31:10.642234       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1203 14:31:15.186572       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I1203 14:31:25.947424       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1203 14:31:25.949481       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1203 14:31:25.981330       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1203 14:31:25.981367       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1203 14:31:25.981392       1 server_linux.go:169] "Using iptables Proxier"
I1203 14:31:26.001729       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1203 14:31:26.002107       1 server.go:483] "Version info" version="v1.31.0"
I1203 14:31:26.002346       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 14:31:26.003628       1 config.go:197] "Starting service config controller"
I1203 14:31:26.008507       1 shared_informer.go:313] Waiting for caches to sync for service config
I1203 14:31:26.004387       1 config.go:104] "Starting endpoint slice config controller"
I1203 14:31:26.008572       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1203 14:31:26.004700       1 config.go:326] "Starting node config controller"
I1203 14:31:26.008585       1 shared_informer.go:313] Waiting for caches to sync for node config
I1203 14:31:26.109084       1 shared_informer.go:320] Caches are synced for node config
I1203 14:31:26.109214       1 shared_informer.go:320] Caches are synced for service config
I1203 14:31:26.109290       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [8ad83a5b9f35] <==
I1203 14:30:23.796014       1 server_linux.go:66] "Using iptables proxy"
I1203 14:30:23.988919       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1203 14:30:23.988976       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1203 14:30:24.038098       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1203 14:30:24.038131       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1203 14:30:24.038156       1 server_linux.go:169] "Using iptables Proxier"
I1203 14:30:24.040336       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1203 14:30:24.040739       1 server.go:483] "Version info" version="v1.31.0"
I1203 14:30:24.040756       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 14:30:24.043634       1 config.go:104] "Starting endpoint slice config controller"
I1203 14:30:24.043649       1 config.go:326] "Starting node config controller"
I1203 14:30:24.044037       1 config.go:197] "Starting service config controller"
I1203 14:30:24.045596       1 shared_informer.go:313] Waiting for caches to sync for node config
I1203 14:30:24.045596       1 shared_informer.go:313] Waiting for caches to sync for service config
I1203 14:30:24.045600       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1203 14:30:24.146391       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1203 14:30:24.146408       1 shared_informer.go:320] Caches are synced for node config
I1203 14:30:24.146432       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [e4aedb3fe4ba] <==
E1203 14:31:15.158566       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.238601       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.238654       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.290604       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.290654       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.510270       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.510328       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.524073       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.524119       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.570808       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.570855       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.712198       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.712238       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.770000       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.770040       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:15.789667       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:15.789702       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:16.061151       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:16.061202       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:16.097545       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:16.097593       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:16.250881       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:16.250928       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:16.449157       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:16.449198       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:16.557057       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:16.557095       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:18.114743       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:18.114787       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:18.974194       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:18.974238       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:19.897929       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:19.897986       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:19.982397       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:19.982469       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:20.657731       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:20.657772       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:20.663299       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:20.663323       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:20.672832       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:20.672861       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.134113       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.134162       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.187931       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.187980       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.276021       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.276058       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.497573       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.497625       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.527382       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.527418       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:21.987315       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:21.987364       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:22.130141       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:22.130182       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:22.286178       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1203 14:31:22.286247       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1203 14:31:25.928193       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1203 14:31:25.928417       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1203 14:31:50.693904       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [f3492dc94cec] <==
I1203 14:30:19.667616       1 serving.go:386] Generated self-signed cert in-memory
W1203 14:30:21.402342       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1203 14:30:21.402379       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1203 14:30:21.402393       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1203 14:30:21.402402       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1203 14:30:21.420585       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1203 14:30:21.422016       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 14:30:21.424338       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1203 14:30:21.424481       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1203 14:30:21.424502       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1203 14:30:21.427158       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1203 14:30:21.527626       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1203 14:30:52.803246       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1203 14:30:52.803370       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E1203 14:30:52.803554       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.473317    1549 status_manager.go:851] "Failed to get status for pod" podUID="1cd4d9de-9208-4ef2-b451-a857d3774d78" pod="monitoring/elasticsearch-coordinating-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.473508    1549 status_manager.go:851] "Failed to get status for pod" podUID="0605bdb0-8a16-4c93-a63b-c0f57dc46981" pod="monitoring/elasticsearch-coordinating-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.473696    1549 status_manager.go:851] "Failed to get status for pod" podUID="3d789cba-845b-46bc-a66e-ae95f4e5877c" pod="monitoring/kibana-68cb46cb69-w4xzl" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/kibana-68cb46cb69-w4xzl\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.473897    1549 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474082    1549 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474258    1549 status_manager.go:851] "Failed to get status for pod" podUID="9f360e10-a5b7-4f43-b468-0ce03dbecc1b" pod="kube-system/coredns-6f6b679f8f-bfmsg" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-bfmsg\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474425    1549 status_manager.go:851] "Failed to get status for pod" podUID="b36bb458-30b4-44a9-ae58-01c19dbcd4e7" pod="monitoring/elasticsearch-master-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474609    1549 status_manager.go:851] "Failed to get status for pod" podUID="58d8e848-3b60-408a-b083-1f997412a652" pod="monitoring/elasticsearch-data-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474781    1549 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.474938    1549 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475120    1549 status_manager.go:851] "Failed to get status for pod" podUID="65a0d6ea-a505-40ee-ae3f-72f49007bbc8" pod="monitoring/elasticsearch-data-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475285    1549 status_manager.go:851] "Failed to get status for pod" podUID="7a0e69a4-b703-45c7-b36b-f57b458fe52e" pod="monitoring/elasticsearch-ingest-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-ingest-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475446    1549 status_manager.go:851] "Failed to get status for pod" podUID="b93b43e4-6543-4ea4-bf3b-fa384cec0b74" pod="monitoring/elasticsearch-master-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475638    1549 status_manager.go:851] "Failed to get status for pod" podUID="b93b43e4-6543-4ea4-bf3b-fa384cec0b74" pod="monitoring/elasticsearch-master-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475813    1549 status_manager.go:851] "Failed to get status for pod" podUID="7a0e69a4-b703-45c7-b36b-f57b458fe52e" pod="monitoring/elasticsearch-ingest-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-ingest-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.475988    1549 status_manager.go:851] "Failed to get status for pod" podUID="b736d07d-3c89-4c70-839f-5c13e4436211" pod="kube-system/kube-proxy-wgz5z" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wgz5z\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.476177    1549 status_manager.go:851] "Failed to get status for pod" podUID="cd9ae1cc-546a-437f-b9a6-7f5ba365980a" pod="monitoring/elasticsearch-ingest-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-ingest-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.476373    1549 status_manager.go:851] "Failed to get status for pod" podUID="1cd4d9de-9208-4ef2-b451-a857d3774d78" pod="monitoring/elasticsearch-coordinating-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.476581    1549 status_manager.go:851] "Failed to get status for pod" podUID="59dc3dd2-b55b-47d3-b5a1-596fe4a5c3fc" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.476776    1549 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.476985    1549 status_manager.go:851] "Failed to get status for pod" podUID="9f360e10-a5b7-4f43-b468-0ce03dbecc1b" pod="kube-system/coredns-6f6b679f8f-bfmsg" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-bfmsg\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.477193    1549 status_manager.go:851] "Failed to get status for pod" podUID="b36bb458-30b4-44a9-ae58-01c19dbcd4e7" pod="monitoring/elasticsearch-master-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.477407    1549 status_manager.go:851] "Failed to get status for pod" podUID="58d8e848-3b60-408a-b083-1f997412a652" pod="monitoring/elasticsearch-data-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.477622    1549 status_manager.go:851] "Failed to get status for pod" podUID="0605bdb0-8a16-4c93-a63b-c0f57dc46981" pod="monitoring/elasticsearch-coordinating-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.477823    1549 status_manager.go:851] "Failed to get status for pod" podUID="3d789cba-845b-46bc-a66e-ae95f4e5877c" pod="monitoring/kibana-68cb46cb69-w4xzl" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/kibana-68cb46cb69-w4xzl\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.478024    1549 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.478213    1549 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.478419    1549 status_manager.go:851] "Failed to get status for pod" podUID="65a0d6ea-a505-40ee-ae3f-72f49007bbc8" pod="monitoring/elasticsearch-data-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:21 minikube kubelet[1549]: I1203 14:31:21.478591    1549 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.174803    1549 scope.go:117] "RemoveContainer" containerID="f722a5fef7175c5bfd433171c6b590e76992634316a7e3aed443852a03145239"
Dec 03 14:31:22 minikube kubelet[1549]: E1203 14:31:22.268161    1549 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="7s"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.381310    1549 status_manager.go:851] "Failed to get status for pod" podUID="59dc3dd2-b55b-47d3-b5a1-596fe4a5c3fc" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.381579    1549 status_manager.go:851] "Failed to get status for pod" podUID="b736d07d-3c89-4c70-839f-5c13e4436211" pod="kube-system/kube-proxy-wgz5z" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wgz5z\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.381826    1549 status_manager.go:851] "Failed to get status for pod" podUID="cd9ae1cc-546a-437f-b9a6-7f5ba365980a" pod="monitoring/elasticsearch-ingest-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-ingest-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.382096    1549 status_manager.go:851] "Failed to get status for pod" podUID="1cd4d9de-9208-4ef2-b451-a857d3774d78" pod="monitoring/elasticsearch-coordinating-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.382321    1549 status_manager.go:851] "Failed to get status for pod" podUID="0605bdb0-8a16-4c93-a63b-c0f57dc46981" pod="monitoring/elasticsearch-coordinating-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-coordinating-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.382557    1549 status_manager.go:851] "Failed to get status for pod" podUID="3d789cba-845b-46bc-a66e-ae95f4e5877c" pod="monitoring/kibana-68cb46cb69-w4xzl" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/kibana-68cb46cb69-w4xzl\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.382783    1549 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.382999    1549 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.383236    1549 status_manager.go:851] "Failed to get status for pod" podUID="9f360e10-a5b7-4f43-b468-0ce03dbecc1b" pod="kube-system/coredns-6f6b679f8f-bfmsg" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-bfmsg\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.383467    1549 status_manager.go:851] "Failed to get status for pod" podUID="b36bb458-30b4-44a9-ae58-01c19dbcd4e7" pod="monitoring/elasticsearch-master-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.383696    1549 status_manager.go:851] "Failed to get status for pod" podUID="58d8e848-3b60-408a-b083-1f997412a652" pod="monitoring/elasticsearch-data-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.383940    1549 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.384172    1549 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.384416    1549 status_manager.go:851] "Failed to get status for pod" podUID="65a0d6ea-a505-40ee-ae3f-72f49007bbc8" pod="monitoring/elasticsearch-data-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-data-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.384654    1549 status_manager.go:851] "Failed to get status for pod" podUID="7a0e69a4-b703-45c7-b36b-f57b458fe52e" pod="monitoring/elasticsearch-ingest-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-ingest-0\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:22 minikube kubelet[1549]: I1203 14:31:22.384877    1549 status_manager.go:851] "Failed to get status for pod" podUID="b93b43e4-6543-4ea4-bf3b-fa384cec0b74" pod="monitoring/elasticsearch-master-1" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/pods/elasticsearch-master-1\": dial tcp 192.168.49.2:8443: connect: connection refused"
Dec 03 14:31:24 minikube kubelet[1549]: I1203 14:31:24.175078    1549 scope.go:117] "RemoveContainer" containerID="abe4da2e2108a638476bafaf6f38bb59d1aa1602807d57d1155b0646366b1cb8"
Dec 03 14:31:24 minikube kubelet[1549]: I1203 14:31:24.175169    1549 scope.go:117] "RemoveContainer" containerID="110ac874f258b96d53ed7f3a7242ad7e77180f755f4dce75b479d135fb1a5c77"
Dec 03 14:31:24 minikube kubelet[1549]: I1203 14:31:24.175239    1549 scope.go:117] "RemoveContainer" containerID="c7209d0e8da8fd99b310ab57fcf7b661ca4364e3aa10d38d85c0a97c3aefedd0"
Dec 03 14:31:24 minikube kubelet[1549]: E1203 14:31:24.243320    1549 desired_state_of_world_populator.go:312] "Error processing volume" err="error processing PVC monitoring/data-elasticsearch-master-0: failed to fetch PVC from API server: Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/monitoring/persistentvolumeclaims/data-elasticsearch-master-0\": dial tcp 192.168.49.2:8443: connect: connection refused" pod="monitoring/elasticsearch-master-0" volumeName="data"
Dec 03 14:31:25 minikube kubelet[1549]: E1203 14:31:25.930537    1549 desired_state_of_world_populator.go:312] "Error processing volume" err="error processing PVC monitoring/data-elasticsearch-master-0: failed to fetch PVC from API server: persistentvolumeclaims \"data-elasticsearch-master-0\" is forbidden: User \"system:node:minikube\" cannot get resource \"persistentvolumeclaims\" in API group \"\" in the namespace \"monitoring\": no relationship found between node 'minikube' and this object" pod="monitoring/elasticsearch-master-0" volumeName="data"
Dec 03 14:31:26 minikube kubelet[1549]: I1203 14:31:26.174713    1549 scope.go:117] "RemoveContainer" containerID="f4c31c1c1e2a9ad2353c5eb49869e5b8c63341d9962c4f88692be1f4cf9fc95b"
Dec 03 14:31:26 minikube kubelet[1549]: I1203 14:31:26.174851    1549 scope.go:117] "RemoveContainer" containerID="61ebb2e97cf6994104bb8f1cc53f4dd86309fe97d815b3dc3771a50c984b7460"
Dec 03 14:31:26 minikube kubelet[1549]: I1203 14:31:26.174977    1549 scope.go:117] "RemoveContainer" containerID="4d3744d85f9247f5593ea5cd140e90655de5bfbfc657122c3bbb5912215c3f08"
Dec 03 14:31:27 minikube kubelet[1549]: I1203 14:31:27.020878    1549 scope.go:117] "RemoveContainer" containerID="4274de8b34d6be67caecef97cc68210442d55087f2dd80cf1b0c32a4bf96994b"
Dec 03 14:31:27 minikube kubelet[1549]: E1203 14:31:27.021518    1549 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(40f5f661ab65f2e4bfe41ac2993c01de)\"" pod="kube-system/kube-controller-manager-minikube" podUID="40f5f661ab65f2e4bfe41ac2993c01de"
Dec 03 14:31:28 minikube kubelet[1549]: I1203 14:31:28.195043    1549 scope.go:117] "RemoveContainer" containerID="cff6d6489d03676cb8ec3d458379fe5c0fddf5f7eb0840049b7fb60013a6c512"
Dec 03 14:31:30 minikube kubelet[1549]: I1203 14:31:30.177400    1549 scope.go:117] "RemoveContainer" containerID="bcc614febeb8d425f0b1404c7121e47ba0a005e3d5246448ed2a3cdcc115c35a"
Dec 03 14:31:39 minikube kubelet[1549]: I1203 14:31:39.174230    1549 scope.go:117] "RemoveContainer" containerID="4274de8b34d6be67caecef97cc68210442d55087f2dd80cf1b0c32a4bf96994b"


==> storage-provisioner [bcc614febeb8] <==
I1203 14:31:06.723998       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1203 14:31:06.727713       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [fdadebee6248] <==
I1203 14:31:30.695627       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1203 14:31:30.790913       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1203 14:31:30.790965       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1203 14:31:48.267359       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1203 14:31:48.267586       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_049bf63e-6695-4a20-9b45-6d74492fdf01!
I1203 14:31:48.269928       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"31407f01-3501-4eab-967b-04525cba0029", APIVersion:"v1", ResourceVersion:"3489", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_049bf63e-6695-4a20-9b45-6d74492fdf01 became leader
I1203 14:31:48.418263       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_049bf63e-6695-4a20-9b45-6d74492fdf01!

@rifaterdemsahin ➜ /workspaces/OpenShiftEventRouter/5_Formula (main) $ 